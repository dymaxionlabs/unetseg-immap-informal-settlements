{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteccion de crecimiento urbano con imagenes del satélite Sentinel - 1\n",
    "\n",
    "## 1_ Entrenamiento del modelo\n",
    "\n",
    "   En este nootbook se realiza el entrenamiento del modelo de de ML. Es una red neuronal convolucional CNN con una arquitectura U-Net. Dicha red requiere un dataset compuesto por imágenes y máscaras para el entrenamiento. Estas ultimas son imagenes binarias que delimitan el objeto de interés, donde los píxeles tienen valor 1 donde está el objeto y 0 donde no. \n",
    "   \n",
    "### Importación de librerias y definición de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unetseg.train import TrainConfig, train\n",
    "from unetseg.evaluate import plot_data_generator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELO_INFO = \"VHd_VVd_VHdVVd\"\n",
    "#bucket con las imagenes de entrenamiento\n",
    "BUCKET_TRAIN = \"gs://dym-indunor-temp/immap/v3_3/training/*.tif\" \n",
    "#creamos la carpeta a donde descargaremos las imagenes\n",
    "BASE_PATH            = \"../\"\n",
    "PATH_S1_IMAGES_TRAIN = f\"{BASE_PATH}imagenes_{MODELO_INFO}/training_images/\"\n",
    "#files con las anotaciones de poligonos y la region de interes para el entrenamiento \n",
    "ANNOTATIONS_FILE_NAME = f\"gt_atlantico_plus_newanotations_4326.geojson\"\n",
    "AOI_FILE_NAME = f\"detections_atlantico_plusnewanotations_buff1km_4326.geojson\"\n",
    "#tamaño de la imagen del dataset\n",
    "SIZE =100\n",
    "STEP_SIZE = 30\n",
    "#nombre del modelo\n",
    "MODELO = f'UNet_TEST_160x160_spe100_3N_spe300_colombia_{MODELO_INFO}.h5'\n",
    "#bucket donde se guarda el modelo\n",
    "SAVE = True\n",
    "BUCKET_MODELO = \"gs://dym-indunor-temp/immap/v3_3/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generacion del data set de entrenamiento\n",
    "\n",
    "   En esta etapa generamos el dataset de entrenamiento mediante el uso de **Satproc**. Esta herramienta permite generar el dataset con todas las caracteristicas necesarias para el modelo: tamaño , cantidad de imagenes, rango de valores, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descarga de imágenes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p $PATH_S1_IMAGES_TRAIN   #crea la carpeta\n",
    "!gsutil -m cp -r $BUCKET_TRAIN   $PATH_S1_IMAGES_TRAIN # descarga de imagenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar que los archivos fueron correctamente descargados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls $PATH_S1_IMAGES_TRAIN # descomentar para ver el listado de imagenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Satproc**\n",
    "\n",
    "Con la herramienta **satproc_extract_chips** se generan, a partir de las imágenes descargadas del bucket, imágenes (chips) generalmente mas pequeñas y máscaras (utilizando las anotaciones de verdad de campo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files   = f'{PATH_S1_IMAGES_TRAIN}/*.tif' #carpeta a las imagenes\n",
    "dataset_folder  = f'{BASE_PATH}dataset_{MODELO_INFO}/data_train/{str(SIZE)}_{str(STEP_SIZE)}/' #carpeta de destino del dataset\n",
    "vector_file     = f'{BASE_PATH}data/shp/gt/'+ ANNOTATIONS_FILE_NAME # archivo vectorial de verdad de campo\n",
    "vector_file_aoi = f'{BASE_PATH}data/shp/' + AOI_FILE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS** : tanto las imagenes como el archivo vectorial deben tener la misma georeferencia, por ejemplo 4326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Ejecutamos satproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!satproc_extract_chips \\\n",
    "                $path_to_files \\\n",
    "                -o  $dataset_folder \\\n",
    "                --size $SIZE \\\n",
    "                --step-size $STEP_SIZE \\\n",
    "                --aoi $vector_file_aoi \\\n",
    "                --labels $vector_file \\\n",
    "                --label-property 'class' \\\n",
    "                --classes 'A' \\\n",
    "                --rescale \\\n",
    "                --rescale-mode values  --min -15 --max 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Los argumentos:\n",
    "\n",
    "* **path_to_files** es la ruta a las imágenes \n",
    "\n",
    "* **o** es la ruta de destino \n",
    "\n",
    "Recomendamos que dicha ruta sea descriptiva, por ejemplo “data_train/120_40/ ” describe : Data_train → datos usados para entrenar; 120_40 → <tamaño de la imagen >_ <tamaño del step-size> \n",
    "\n",
    "* **size** tamaño de las imágenes resultantes o chips (las imágenes son cuadradas) \n",
    "* **step-size** paso del proceso. Debe ser menor o igual a *size*. Si *step-size* es igual que el *size* entonces no hay overlap en las imágenes resultantes. \n",
    "\n",
    "En ocasiones es útil para el entrenamiento generar los chips con un overlap de este modo tenemos más datos para entrenar. \n",
    "\n",
    "\n",
    "* **label-property** nombre del campo donde se define cada categoría (solo se usa para el entrenamiento) \n",
    "\n",
    "* **classes** nombres de las clases (como aparecen en el geojson), separados por espacios\n",
    "\n",
    "* **aoi** ruta al archivo vectorial donde están definidas las anotaciones. Al definir una region de interés solo se procesan las imágenes que interceptan esas anotaciones.\n",
    "\n",
    "* **rescale** lleva los valores de las bandas a 0-255 \n",
    "\n",
    "Este comando va a generar dos carpetas en la ruta de destino : “images” y “extent”. Los archivos de la primera van a ser de tipo Tiff de 3 bandas (rgb) y los de la segunda van a ser también de tipo Tiff, pero de N bandas, donde cada una es una máscara binaria. N representa el número de clases, en este caso sólo una. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Generamos el entrenamiento del modelo utilizando los datasets creados en el paso previo. El modelo es una red neuronal CNN basado en la arquitectura U-Net. Este considera las imágenes y las máscaras binarias como inputs y genera una imagen con la probabilidad de encontrar al objeto de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa debemos definir la configuración del modelo de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Obs*: Es util usar un nombre para el archivo de pesos que de información sobre los parametros de entrenamiento. por ejemplo: < modelo >_< proyecto >_< dim_de_las_imagenes >_< size >_< step_size >_< step_per_epoch >.h5 o similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(width      = 160,  #  tamaño de la imagen procesada por la UNet (debe ser multiplos de 16 , por ej 160, 320,etc; y no menor a 80)\n",
    "                     height     = 160,\n",
    "                     n_channels = 3,  #  número de canales de la imagen, rgb -> 3\n",
    "                     n_classes  = 1, # número de clases a clasificar\n",
    "                     apply_image_augmentation = True, #  si es True , amplia el dataset generando imagenes nuevas a partir de pequeñas variaciones de las ya existentes (rotación,)\n",
    "                     seed       = 42,\n",
    "                     epochs     = 20, # Cantidad de veces que el dataset entero puede pasar por el proceso de entrenamiento\n",
    "                     batch_size = 16, #cantidad de datos que se procesan por vez, puede ser limitado por la memoria de gpu disponible (debe ser multiplo de 16)\n",
    "                     steps_per_epoch          = 300, #  típicamente debe ser igual al numero de imágenes / el batch_size, si es mayor incrementara el número de imágenes generadas con image augmentation\n",
    "                     early_stopping_patience  = 5, # a medida que entrena se guardan los resultados del entrenamiento despues de cada epoch, si el error no varió luego de ¿¿10 ?? iteraciones , se corta el proceso porque se entiende que el error ya disminuyó significativamente \n",
    "                     validation_split   = 0.2, # se divide la muestra en training (80% de los datos) y validation (20% de los datos) para calcular el error durante el proceso de entrenamiento\n",
    "                     test_split         = 0.1, # Cantidad de imágenes del dataset\n",
    "                     images_path        = dataset_folder,#ruta a las imágenes generadas con Satproc\n",
    "                     model_path         = os.path.join(BASE_PATH,'data/weights/', MODELO),#  ruta del modelo entrenado\n",
    "                     model_architecture = 'unet',\n",
    "                     evaluate           = True  ,\n",
    "                     class_weights      = [1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar alguna de las imágenes y máscaras del dataset de entrenamiento. A la izquierda se muestra la imágen y a la derecha la máscara "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_data_generator(num_samples=3, fig_size=(10, 10), train_config=config,img_ch = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_config = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos las métricas generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(res_config.history['loss'])\n",
    "plt.plot(res_config.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(res_config.history['mean_io_u'])\n",
    "plt.plot(res_config.history['val_mean_io_u'])\n",
    "plt.title('mean_iou')\n",
    "plt.ylabel('val_mean_iou')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE: \n",
    "    !gsutil cp -r config.model_path $BUCKET_MODELO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNKlws8+6PcuvJrSWdSmTxJ",
   "include_colab_link": true,
   "name": "2 - Entrenamiento",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
